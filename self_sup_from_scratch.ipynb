{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_sup_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "onSNetsTffPC"
      },
      "source": [
        "!pip install kornia\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H7jK1ziPlAH",
        "outputId": "995abee2-705a-4d73-f38c-89d192b2c10e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc3xZcZLwmV2"
      },
      "source": [
        "cat_to_idx = {'Cassava bacterial blight (cbb)': 0,\n",
        "              'Cassava brown streak disease (cbsd)': 1,\n",
        "              'Cassava green mottle (cgm)': 2,\n",
        "              'Cassava mosaic disease (cmd)': 3,\n",
        "              'Healthy': 4}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLcE2ISDwxQO"
      },
      "source": [
        "import random\n",
        "from typing import Callable, Tuple\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "from kornia import augmentation as aug\n",
        "from kornia import filters\n",
        "from kornia.geometry import transform as tf\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "from itertools import chain\n",
        "from typing import Dict, List\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torch import optim\n",
        "import torch.nn.functional as f\n",
        "\n",
        "from torchvision.models import resnet18, resnet34, resnet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkTxCwi5Pk9s"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, transforms, mapping = cat_to_idx):\n",
        "        self.data_path = data_path\n",
        "        self.transforms = transforms\n",
        "        self.mapping = cat_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        im = Image.open(self.data_path.loc[idx, 'file_name'])\n",
        "        label = int(self.mapping[self.data_path.loc[idx, 'classes']])\n",
        "\n",
        "        if self.transforms:\n",
        "            im = self.transforms(im)\n",
        "        return im, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO_dE2T6RqZS"
      },
      "source": [
        "train_data = pd.read_csv('/content/remo_train.csv')\n",
        "validation_data = pd.read_csv('/content/remo_valid.csv')\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "val_transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "#train_dl = DataLoader(CustomDataset(data_path=train_data, transforms=train_transforms), batch_size=128, num_workers=2, pin_memory=True)\n",
        "#val_dl = DataLoader(CustomDataset(data_path=validation_data, transforms=val_transforms), batch_size = 50, num_workers=2, pin_memory=True)\n",
        "\n",
        "train_dl = DataLoader(CustomDataset(data_path=train_data, transforms=train_transforms), batch_size=128, num_workers=2)\n",
        "val_dl = DataLoader(CustomDataset(data_path=validation_data, transforms=val_transforms), batch_size = 50, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73oizFJPk1n"
      },
      "source": [
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn: Callable, p: float):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return x if random.random() > self.p else self.fn(x)\n",
        "\n",
        "\n",
        "def default_augmentation(image_size: Tuple[int, int] = (224, 224)) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        tf.Resize(size=image_size),\n",
        "        RandomApply(aug.ColorJitter(0.8, 0.8, 0.8, 0.2), p=0.8),\n",
        "        aug.RandomGrayscale(p=0.2),\n",
        "        aug.RandomHorizontalFlip(),\n",
        "        RandomApply(filters.GaussianBlur2d((3, 3), (1.5, 1.5)), p=0.1),\n",
        "        aug.RandomResizedCrop(size=image_size),\n",
        "        aug.Normalize(\n",
        "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "            std=torch.tensor([0.229, 0.224, 0.225]),\n",
        "        ),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKzYHdqOPk4i"
      },
      "source": [
        "def mlp(dim: int, projection_size: int = 256, hidden_size: int = 4096) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, hidden_size),\n",
        "        nn.BatchNorm1d(hidden_size),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, projection_size),\n",
        "    )\n",
        "\n",
        "\n",
        "class EncoderWrapper(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        projection_size: int = 256,\n",
        "        hidden_size: int = 4096,\n",
        "        layer: Union[str, int] = -2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.projection_size = projection_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer = layer\n",
        "\n",
        "        self._projector = None\n",
        "        self._projector_dim = None\n",
        "        self._encoded = torch.empty(0)\n",
        "        self._register_hook()\n",
        "\n",
        "    @property\n",
        "    def projector(self):\n",
        "        if self._projector is None:\n",
        "            self._projector = mlp(\n",
        "                self._projector_dim, self.projection_size, self.hidden_size\n",
        "            )\n",
        "        return self._projector\n",
        "    \n",
        "    def _hook(self, _, __, output):\n",
        "        output = output.flatten(start_dim=1)\n",
        "        if self._projector_dim is None:\n",
        "            # If we haven't already, measure the output size\n",
        "            self._projector_dim = output.shape[-1]\n",
        "\n",
        "        # Project the output to get encodings\n",
        "        self._encoded = self.projector(output)\n",
        "\n",
        "    def _register_hook(self):\n",
        "        if isinstance(self.layer, str):\n",
        "            layer = dict([*self.model.named_modules()])[self.layer]\n",
        "        else:\n",
        "            layer = list(self.model.children())[self.layer]\n",
        "\n",
        "        layer.register_forward_hook(self._hook)\n",
        "        \n",
        "    # ------------------- End hooks methods ----------------------\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Pass through the model, and collect 'encodings' from our forward hook!\n",
        "        _ = self.model(x)\n",
        "        return self._encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLvtMAczPk7I"
      },
      "source": [
        "def normalized_mse(x: Tensor, y: Tensor) -> Tensor:\n",
        "    x = f.normalize(x, dim=-1)\n",
        "    y = f.normalize(y, dim=-1)\n",
        "    return torch.mean(2 - 2 * (x * y).sum(dim=-1))\n",
        "\n",
        "\n",
        "class BYOL(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        image_size: Tuple[int, int] = (96, 96),\n",
        "        hidden_layer: Union[str, int] = -2,\n",
        "        projection_size: int = 256,\n",
        "        hidden_size: int = 4096,\n",
        "        augment_fn: Callable = None,\n",
        "        beta: float = 0.99,\n",
        "        **hparams,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.augment = default_augmentation(image_size) if augment_fn is None else augment_fn\n",
        "        self.beta = beta\n",
        "        self.encoder = EncoderWrapper(\n",
        "            model, projection_size, hidden_size, layer=hidden_layer\n",
        "        )\n",
        "        self.predictor = nn.Linear(projection_size, projection_size, hidden_size)\n",
        "        self.hparams = hparams\n",
        "        self._target = None\n",
        "\n",
        "        # Perform a single forward pass, which initializes the 'projector' in our \n",
        "        # 'EncoderWrapper' layer.\n",
        "        self.encoder(torch.zeros(2, 3, *image_size))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.predictor(self.encoder(x))\n",
        "\n",
        "    @property\n",
        "    def target(self):\n",
        "        if self._target is None:\n",
        "            self._target = deepcopy(self.encoder)\n",
        "        return self._target\n",
        "\n",
        "    def update_target(self):\n",
        "        for p, pt in zip(self.encoder.parameters(), self.target.parameters()):\n",
        "            pt.data = self.beta * pt.data + (1 - self.beta) * p.data\n",
        "\n",
        "    # --- Methods required for PyTorch Lightning only! ---\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = getattr(optim, self.hparams.get(\"optimizer\", \"Adam\"))\n",
        "        lr = self.hparams.get(\"lr\", 1e-4)\n",
        "        weight_decay = self.hparams.get(\"weight_decay\", 1e-6)\n",
        "        return optimizer(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    def training_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n",
        "        x = batch[0]\n",
        "        with torch.no_grad():\n",
        "            x1, x2 = self.augment(x), self.augment(x)\n",
        "\n",
        "        pred1, pred2 = self.forward(x1), self.forward(x2)\n",
        "        with torch.no_grad():\n",
        "            targ1, targ2 = self.target(x1), self.target(x2)\n",
        "        loss = (normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1)) / 2\n",
        "\n",
        "        self.log(\"train_loss\", loss.item())\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n",
        "        x = batch[0]\n",
        "        x1, x2 = self.augment(x), self.augment(x)\n",
        "        pred1, pred2 = self.forward(x1), self.forward(x2)\n",
        "        targ1, targ2 = self.target(x1), self.target(x2)\n",
        "        loss = (normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1)) / 2\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_epoch_end(self, outputs: List[Dict]) -> Dict:\n",
        "        val_loss = sum(x[\"loss\"] for x in outputs) / len(outputs)\n",
        "        self.log(\"val_loss\", val_loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6QRdBJFrYKj"
      },
      "source": [
        "model = resnet50(pretrained=True)\n",
        "byol = BYOL(model, image_size=(96, 96))\n",
        "\n",
        "trainer = pl.Trainer(gpus=1)\n",
        "\n",
        "lr_finder = trainer.tuner.lr_find(byol, train_dl, val_dl)\n",
        "\n",
        "fig = lr_finder.plot(); fig.show()\n",
        "suggested_lr = lr_finder.suggestion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNZ-GnHgsGWE"
      },
      "source": [
        "suggested_lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXWDmsVPlFz"
      },
      "source": [
        "from torchvision.models import resnet18, resnet34, resnet50\n",
        "model = resnet50(pretrained=True)\n",
        "byol = BYOL(model, lr = 0.001, image_size=(96, 96))\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50, \n",
        "    gpus=1,\n",
        "    # Batch size of 2048 matches the BYOL paper\n",
        "    accumulate_grad_batches=2048 // 50,\n",
        "    weights_summary=None\n",
        ")\n",
        "\n",
        "lr_finder = trainer.tuner.lr_find(byol, train_dl, val_dl)\n",
        "\n",
        "fig = lr_finder.plot(); fig.show()\n",
        "suggested_lr = lr_finder.suggestion()\n",
        "\n",
        "#hparams.lr = suggested_lr\n",
        "#trainer.fit(byol, train_dl, val_dl, hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RrkBgRgZnPJ"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}